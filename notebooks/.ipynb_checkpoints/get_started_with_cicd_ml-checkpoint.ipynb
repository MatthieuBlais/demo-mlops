{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "victorian-twelve",
   "metadata": {},
   "source": [
    "# Get started with CI/CD for ML projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-damages",
   "metadata": {},
   "source": [
    "### Concepts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-farming",
   "metadata": {},
   "source": [
    "Software engineers may be familiar with the concepts of continuous integration and continuous delivery. The basic flow consists of pushing code to a Git repository, then this event triggers a job to test the code and build the application in an automated way. One of the most famous open-source tool is Jenkins, but Cloud providers also have their own services, such\n",
    "as Cloud Build for GCP, or CodeBuild/CodePipeline for AWS. One of the main advantages of CI/CD is the automation of all the deployment tasks, which shorten software development iterations.\n",
    "\n",
    "CI/CD for data-science is becoming a norm. Deploying models to production is not easy and DevOps engineers are bringing their expertise to the ML teams to simplify the process. Many of\n",
    "the lessons learnt by software engineering teams can be re-used, at the exception that in addition of testing code, ML teams also need to test data and evaluate models. A CI/CD workflow\n",
    "for data-science could look like this:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-decade",
   "metadata": {},
   "source": [
    "<img src=\"cicd.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-environment",
   "metadata": {},
   "source": [
    "\n",
    "This picture is extracted from this interesting Google blog post: https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning.\n",
    "If you want to know more about it, and MLOps in general, I encourage you to read it.\n",
    "\n",
    "In this blog we will setup a simple Continuous Integration pipeline that you can re-use across projects. As a first step, it will be very similar to a standard Software Engineering pipeline, and in a future article we will try to enhance it and add more features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-treat",
   "metadata": {},
   "source": [
    "### What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-invasion",
   "metadata": {},
   "source": [
    "In the next section, we set up a simple pipeline continuous integration flow. This would cover the first steps of the deployment flow shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-superintendent",
   "metadata": {},
   "source": [
    "<img src=\"architecture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-cardiff",
   "metadata": {},
   "source": [
    "A ML pipeline is composed of a few components for data preparation, training, model validation, inference, etc. A popular pattern is to package each component in containers as it's a good\n",
    "strategy to be able to reproduce results without dependency on the hardware/OS it runs on. Wether you are running your containers on-prem, or in the Cloud, it shouldn't matter as your code will always\n",
    "be executed in the same Docker environment. Today, most of ML frameworks chooses this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-scholar",
   "metadata": {},
   "source": [
    "We will start with a demo Git repository on GitHub. We will create two dummy components with a corresponding Dockerfile and some unit tests. As any software product, your code deserves \n",
    "to be tested and starting writing unit tests from the beginning is a good habit to have. Then, we will configure an event on Git push to trigger a build job, in both GCP and AWS. In a\n",
    "real-world setting, you may not need to deploy in multiple Cloud providers, but this is just for demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-mercy",
   "metadata": {},
   "source": [
    "### Let's get started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-password",
   "metadata": {},
   "source": [
    "First, create your own GitHub repository. In this example, I use my repo https://github.com/MatthieuBlais/demo-mlops.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-nomination",
   "metadata": {},
   "source": [
    "#### Repository structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-application",
   "metadata": {},
   "source": [
    "You may start your project by analysing data on a notebook. It's important to add them to your repository as it helps others to quickly \n",
    "explore and understand your experiments. However, you may want to avoid deploying Jupyter notebooks in production and instead, organise your code into components. In addition of that, \n",
    "you may also have some Dockerfile needed to build your app. With all these different kind of files, it can quickly become messy and difficul to manage. To avoid that, following a clear\n",
    "repository structure is important and can also help to automate the deployment process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-bench",
   "metadata": {},
   "source": [
    "In this example, I've made the choice or creating a folder structure like this:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "written-iceland",
   "metadata": {},
   "source": [
    "demo-mlops\n",
    "|---components/\n",
    "    |---dataprep/\n",
    "        |---app/\n",
    "            |---tests/\n",
    "            |---main.pt\n",
    "        |---components.yaml\n",
    "        |---Dockerfile\n",
    "        |---requirements.txt\n",
    "    |---modeltraining/\n",
    "        |---app/\n",
    "            |---tests/\n",
    "            |---main.pt\n",
    "        |---components.yaml\n",
    "        |---Dockerfile\n",
    "        |---requirements.txt    \n",
    "|---notebooks/\n",
    "|---deployment/\n",
    "    |---build-dockers.sh\n",
    "    |---run-tests.sh\n",
    "|---README.md\n",
    "|---cloudbuild.yaml\n",
    "|---dev-requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-privacy",
   "metadata": {},
   "source": [
    "In the \"components\" folder, I can add all the components I need for my ML pipeline. Each component comes with its own testable code (app folder) a component definition (components.yaml - very similar to Kubeflow definition), a Dockerfile and a requirements.txt.\n",
    "In the \"deployment\" folder, I keep the scripts needed for the CI/CD pipeline. On the repo, I've created two folders, one for AWS and one for GCP, but as mentioned you probably need only one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-series",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-dispute",
   "metadata": {},
   "source": [
    "Before setting up the CI pipeline, let's confirm everything is running as expected. For each component, try to build the Docker image locally:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "above-russia",
   "metadata": {},
   "source": [
    "## In a component subfolder:\n",
    "docker build . -t dataprepdemo\n",
    "docker run --entrypoint=python test main.py --data-location s3://mylocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-pharmacology",
   "metadata": {},
   "source": [
    "You should be able to successfully build your docker image. You can also try to test the (very simple) code using pytest:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "efficient-singer",
   "metadata": {},
   "source": [
    "pytest ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-nickel",
   "metadata": {},
   "source": [
    "Now that everything is ready, let's configure the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-return",
   "metadata": {},
   "source": [
    "### GCP setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-reconstruction",
   "metadata": {},
   "source": [
    "When we push to our GitHub repository, we want to trigger a Cloud Build job to test our code and build our component's Dockers. The first step is to configure this Cloud Build trigger. The best way to proceed is to refer to the official documentation: https://cloud.google.com/build/docs/automating-builds/run-builds-on-github. After following this guide, if you try to push code to you repository, you should notice a new build starting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-floating",
   "metadata": {},
   "source": [
    "#### First pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-serbia",
   "metadata": {},
   "source": [
    "You may notice the codebuild.yaml in the folder deployment/gcp/. This file describes the steps executed by Cloud Build. Let's keep it simple for now. We have 2 components, we want to run\n",
    "pytest and get the coverage for each of them (2 steps), then we want to build the Docker images (2 steps) and push them to GCR (2 steps). Total, we have 6 steps. 2 of them require a Python environment and 4 of them a docker environment. Our first cloudbuild.yaml can look like this:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "nonprofit-maintenance",
   "metadata": {},
   "source": [
    "steps:\n",
    "- name: 'python:3.8-slim'\n",
    "  entrypoint: /bin/sh\n",
    "  args:\n",
    "    - -c\n",
    "    - 'cd components/dataprep/ && pip install -r requirements.txt && pip install pytest pytest-cov && python -m pytest --cov app/'\n",
    "- name: 'python:3.8-slim'\n",
    "  entrypoint: /bin/sh\n",
    "  args:\n",
    "    - -c\n",
    "    - 'cd components/modeltraining/ && pip install -r requirements.txt && pip install pytest pytest-cov && python -m pytest --cov app/'\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: [ 'build', '-t', 'gcr.io/$PROJECT_ID/$REPO_NAME/dataprep', 'components/dataprep/' ]\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['push', 'gcr.io/$PROJECT_ID/$REPO_NAME/dataprep']\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: [ 'build', '-t', 'gcr.io/$PROJECT_ID/$REPO_NAME/modeltraining', 'components/modeltraining/' ]\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['push', 'gcr.io/$PROJECT_ID/$REPO_NAME/modeltraining']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-nicaragua",
   "metadata": {},
   "source": [
    "We use a Python image to run pytest for our two components. Then, we use one of the default Cloud Build docker image to build our images. Note that we set the tags to be able to easily push\n",
    "the images to GCR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-economics",
   "metadata": {},
   "source": [
    "#### Enhancing the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-universe",
   "metadata": {},
   "source": [
    "For now we have 2 components and our Cloud Build flow has 6 steps. This means that if we have 10 components, we will have 30 steps. However, we can notice that all the commands are very\n",
    "similar except the name of the components. We can simplify the flow by writing two bash scripts (one for pytest and one for the docker images) iterating over the list of components and \n",
    "executing the same commands for each of them."
   ]
  },
  {
   "cell_type": "raw",
   "id": "likely-algorithm",
   "metadata": {},
   "source": [
    "steps:\n",
    "- name: 'python:3.8-slim'\n",
    "  entrypoint: /bin/sh\n",
    "  args:\n",
    "    - deployment/gcp/run-tests.sh\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  entrypoint: /bin/bash\n",
    "  args:\n",
    "    - deployment/gcp/build-dockers.sh\n",
    "  env:\n",
    "    - 'BRANCH_NAME=$BRANCH_NAME'\n",
    "    - 'PROJECT_ID=$PROJECT_ID'\n",
    "    - 'SHORT_SHA=$SHORT_SHA'\n",
    "    - 'REPO_NAME=$REPO_NAME'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-macedonia",
   "metadata": {},
   "source": [
    "This cloudbuild definition will remain the same whatever the number of components we have. Note that we kept the same images, python:3.8-slim and docker. If you go through the bash scripts,\n",
    "you may notice we add two tags to our Docker images:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "narrative-domestic",
   "metadata": {},
   "source": [
    "image_name=\"${folder%?}\"\n",
    "docker build -t gcr.io/$PROJECT_ID/demo-mlops/$image_name:$BRANCH_NAME-$SHORT_SHA -t gcr.io/$PROJECT_ID/demo-mlops/$image_name:$BRANCH_NAME-latest ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-vegetation",
   "metadata": {},
   "source": [
    "This is to keep the GCR repository tidy and to easily identify what is the latest image for each branch. You can obviously choose a different method to organise the repository but it's important\n",
    "to avoid the following situation. Let's say a ML pipeline in production uses the docker image **gcr.io/PROJECT_ID/demo-mlops/component:latest**. You want to add a new feature to this component\n",
    "and you push a new Dockerfile to Github. It means the new \"latest\" image will be the one built during your latest push. Next time your ML pipeline in production is triggered, it will\n",
    "also use this new image, but you haven't done any proper testing to know if something is going to break or not. This is a dangerous situation. Adding the branch name in the image tag can help\n",
    "to avoid this issue. \n",
    "If you push to your dev branch, the image will be tagged with **gcr.io/PROJECT_ID/demo-mlops/component:dev-latest**, but as your prod environment (which is using the \"master\" branch) uses the \n",
    "    image **gcr.io/PROJECT_ID/demo-mlops/component:master-latest**, there won't be any conflict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-general",
   "metadata": {},
   "source": [
    "#### (Optional) One more thing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-request",
   "metadata": {},
   "source": [
    "We have already achieved what we wanted to do and we could stop here, but there is one more thing... During the build, we do not save any artifact. If we want to\n",
    "know the pytest coverage or the Docker images that have been built, we must read the logs to find out. This is not very nice. Instead let's save the reports to Google Cloud Storage! "
   ]
  },
  {
   "cell_type": "raw",
   "id": "genetic-entertainment",
   "metadata": {},
   "source": [
    "## Create your bucket.\n",
    "gsutil mb -l LOCATION gs://PROJECT_ID-mlops-deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-finger",
   "metadata": {},
   "source": [
    "We save the artifacts in this location: cloudbuild/REPO_NAME/BRANCH_NAME/BUILD_ID/. To do that we add an artifact section to our two Cloud Build steps."
   ]
  },
  {
   "cell_type": "raw",
   "id": "radical-finance",
   "metadata": {},
   "source": [
    "artifacts:\n",
    "    objects:\n",
    "      location: 'gs://$PROJECT_ID-mlops-deployments/cloudbuild/$REPO_NAME/$BRANCH_NAME/$BUILD_ID/'\n",
    "      paths: ['_artifacts/*']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-pleasure",
   "metadata": {},
   "source": [
    "You must also update the bash scripts to save all the artifacts you want to save in the folder \"_artifacts/\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-credit",
   "metadata": {},
   "source": [
    "Push your changes and here we are, you should see your artifacts being uploaded at the end of your build:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "american-camel",
   "metadata": {},
   "source": [
    "Artifacts will be uploaded to gs://freeldom-mlops-deployments using gsutil cp\n",
    "_artifacts/*: Uploading path....\n",
    "Copying file://_artifacts/dataprep-coverage.json [Content-Type=application/json]...\n",
    "Copying file://_artifacts/images.txt [Content-Type=text/plain]...\n",
    "Copying file://_artifacts/modeltraining-coverage.json [Content-Type=application/json]..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-isaac",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-specialist",
   "metadata": {},
   "source": [
    "We have seen how to create a simple Continuous Integration for your ML projects. We have used GCP Cloud Build and AWS CodeBuild, but there are other tools and products that you can explore.\n",
    "One important condition to enable CI is to have a proper repository structure. This demo is a just an example to get you started. You can enhance it in many ways. For example, the scripts\n",
    "used to test the code and build the docker images are currently in the source repository. This is something you would change if you work with a big team because most of your projects would\n",
    "use the same scripts and they will get copy-pasted in many repositories. The day you want to update something in one of them, you will have to apply the change to all the projects! Instead,\n",
    "you could have a process to store the scripts on GS/S3 and download them during the build execution. \n",
    "Another area that could be improved is the handling of artifacts. Currently, we just upload them to GS/S3, but during a code review, the engineers will still have to go to the buckets to \n",
    "check the results. Instead of that you could have one more step during the build to summarize the results and save them into a database. Based on these results, you could also have an automated \n",
    "\"first code review\", that decide if a PR should be closed or not based on the pytest coverage or other findings. \n",
    "Start small and improve over time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-asian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
