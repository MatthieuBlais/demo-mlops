{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "decent-argentina",
   "metadata": {},
   "source": [
    "# Get started with CI/CD for ML projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-squad",
   "metadata": {},
   "source": [
    "### Concepts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-software",
   "metadata": {},
   "source": [
    "Software engineers may be familiar with the concepts of continuous integration and continuous delivery. The basic flow consists of pushing code to a Git repository. This event triggers a job to test the code and build the application in an automated way. One of the most famous open-source tool is Jenkins, but Cloud providers also have their own services, such\n",
    "as Cloud Build for GCP, or CodeBuild/CodePipeline for AWS. One of the main advantages of CI/CD is the automation of all the deployment tasks, which shorten software development iterations.\n",
    "\n",
    "CI/CD for data-science is becoming a norm. Deploying models to production is not easy and DevOps engineers are bringing their expertise to the ML teams to simplify the process. Many of\n",
    "the lessons learnt by software engineering teams can be re-used, at the exception that in addition of testing code, ML teams also need to test data and evaluate models. A CI/CD workflow\n",
    "for data-science could look like this:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-giving",
   "metadata": {},
   "source": [
    "<img src=\"cicd.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-documentary",
   "metadata": {},
   "source": [
    "\n",
    "This picture is extracted from this interesting Google blog post: https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning.\n",
    "If you want to know more about it, and MLOps in general, I encourage you to read it.\n",
    "\n",
    "In this blog we will setup a simple Continuous Integration pipeline that you can re-use across projects. As a first step, it will be very similar to a standard Software Engineering pipeline, and in a future article we will try to enhance it and add more features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-windsor",
   "metadata": {},
   "source": [
    "### What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-sodium",
   "metadata": {},
   "source": [
    "In the next section, we set up a simple pipeline continuous integration flow. This would cover the first steps of the deployment flow shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-concentration",
   "metadata": {},
   "source": [
    "<img src=\"architecture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-baptist",
   "metadata": {},
   "source": [
    "A ML pipeline is composed of a few components for data preparation, training, model validation, inference, etc. A popular pattern is to package each component in containers as it's a good\n",
    "strategy to be able to reproduce results without dependency on the hardware/OS it runs on. Wether you are running your containers on-prem, or in the Cloud, it shouldn't matter as your code will always\n",
    "be executed in the same Docker environment. Today, most of ML frameworks chooses this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-fundamental",
   "metadata": {},
   "source": [
    "We will start with a demo Git repository on GitHub. We will create two dummy components with a corresponding Dockerfile and some unit tests. As any software product, your code deserves \n",
    "to be tested and starting writing unit tests from the beginning is a good habit to have. Then, we will configure an event on Git push to trigger a build job, in both GCP and AWS. In a\n",
    "real-world setting, you may not need to deploy in multiple Cloud providers, but this is just for demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-antarctica",
   "metadata": {},
   "source": [
    "### Let's get started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-infection",
   "metadata": {},
   "source": [
    "First, create your own GitHub repository. In this example, I use my repo https://github.com/MatthieuBlais/demo-mlops.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-overall",
   "metadata": {},
   "source": [
    "#### Repository structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-commonwealth",
   "metadata": {},
   "source": [
    "Using notebooks to analyse data is very common, especially during the exploration phase. Adding them to your repository is important as it helps others to quickly understand your experiments. However, when building the end-to-end pipeline, the code must be re-organised to be \"production grade\", including unit and integration tests. As a pipeline can have multiple steps (components), it's a good practice to follow a common repository structure across your projects to make them easy to maintain and easy to deploy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-double",
   "metadata": {},
   "source": [
    "In this blog, and the following ones, we will follow a structure like this one:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-punishment",
   "metadata": {},
   "source": [
    "```\n",
    "demo-mlops\n",
    "|---components/\n",
    "    |---dataprep/\n",
    "        |---app/\n",
    "            |---tests/\n",
    "            |---main.pt\n",
    "        |---components.yaml\n",
    "        |---Dockerfile\n",
    "        |---requirements.txt\n",
    "    |---modeltraining/\n",
    "        |---app/\n",
    "            |---tests/\n",
    "            |---main.pt\n",
    "        |---components.yaml\n",
    "        |---Dockerfile\n",
    "        |---requirements.txt    \n",
    "|---notebooks/\n",
    "|---deployment/\n",
    "    |---build-dockers.sh\n",
    "    |---run-tests.sh\n",
    "|---README.md\n",
    "|---cloudbuild.yaml\n",
    "|---dev-requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-force",
   "metadata": {},
   "source": [
    "In the \"components\" folder, we add all the components needed for the ML pipeline. Each component comes with its own testable code (app folder) a component definition (components.yaml - very similar to Kubeflow definition), a Dockerfile and a requirements.txt.\n",
    "In the \"deployment\" folder, we keep the scripts needed for the CI/CD pipeline. In this sample repo, we look at \"building\" our pipeline in both AWS and GCP, so you will see two subfolders \"aws\" and \"gcp\" in this deployment folder. For real-world project, you probably don't need both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-opera",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-collect",
   "metadata": {},
   "source": [
    "Before setting up the CI pipeline, let's confirm everything is running as expected. For each component, try to build the Docker image locally:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-blogger",
   "metadata": {},
   "source": [
    "```\n",
    "## In a component subfolder:\n",
    "docker build . -t dataprepdemo\n",
    "docker run --entrypoint=python dataprepdemo main.py --data-location s3://mylocation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-clerk",
   "metadata": {},
   "source": [
    "You should be able to successfully build your docker image. You can also try to test the (very simple) code using pytest:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-windows",
   "metadata": {},
   "source": [
    "```\n",
    "pytest .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-plaza",
   "metadata": {},
   "source": [
    "Now that everything is ready, let's configure the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-sauce",
   "metadata": {},
   "source": [
    "### GCP setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-reaction",
   "metadata": {},
   "source": [
    "Let's go through the main steps with GCP before looking at AWS. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-terminal",
   "metadata": {},
   "source": [
    "When we push changes to our GitHub repository, we trigger a Cloud Build job that tests our code and builds our component's Docker images. The first step is to configure this Cloud Build trigger. The best way to proceed is to refer to the official documentation: https://cloud.google.com/build/docs/automating-builds/run-builds-on-github. If correctly configured, you should notice new builds starting every time you push something to your repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-sweet",
   "metadata": {},
   "source": [
    "#### First pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-anatomy",
   "metadata": {},
   "source": [
    "In the root folder of the repo, there is a file codebuild.yaml. This file describes the steps that Cloud Build has to execute. Let's keep our first version simple for now. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-envelope",
   "metadata": {},
   "source": [
    "We have two components, and when we push to our repo we want to execute the unit tests (1 step each = 2 steps), build the Docker images (2 steps) and push them to GCR (2 steps). To run the unit tests, we need a Python environment and the remaining tasks require in a docker environment. Our first cloudbuild.yaml looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-sunglasses",
   "metadata": {},
   "source": [
    "```\n",
    "steps:\n",
    "- name: 'python:3.8-slim'\n",
    "  entrypoint: /bin/sh\n",
    "  args:\n",
    "    - -c\n",
    "    - 'cd components/dataprep/ && pip install -r requirements.txt && pip install pytest pytest-cov && python -m pytest --cov app/'\n",
    "- name: 'python:3.8-slim'\n",
    "  entrypoint: /bin/sh\n",
    "  args:\n",
    "    - -c\n",
    "    - 'cd components/modeltraining/ && pip install -r requirements.txt && pip install pytest pytest-cov && python -m pytest --cov app/'\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: [ 'build', '-t', 'gcr.io/$PROJECT_ID/$REPO_NAME/dataprep', 'components/dataprep/' ]\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['push', 'gcr.io/$PROJECT_ID/$REPO_NAME/dataprep']\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: [ 'build', '-t', 'gcr.io/$PROJECT_ID/$REPO_NAME/modeltraining', 'components/modeltraining/' ]\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['push', 'gcr.io/$PROJECT_ID/$REPO_NAME/modeltraining']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-simple",
   "metadata": {},
   "source": [
    "#### Enhancing the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-guidance",
   "metadata": {},
   "source": [
    "We have 2 components and our Cloud Build flow has 6 steps. If we had 10 components, our build would have 30 steps and it would become complex. We can notice that all the commands are very similar and only the component name changes. We know that all components are in the same folders, so we can automate the build by writing two bash scripts (one for pytest and one for the docker images) iterating over our components and executing the commands we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-vulnerability",
   "metadata": {},
   "source": [
    "```\n",
    "steps:\n",
    "- name: 'python:3.8-slim'\n",
    "  entrypoint: /bin/sh\n",
    "  args:\n",
    "    - deployment/gcp/run-tests.sh\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  entrypoint: /bin/bash\n",
    "  args:\n",
    "    - deployment/gcp/build-dockers.sh\n",
    "  env:\n",
    "    - 'BRANCH_NAME=$BRANCH_NAME'\n",
    "    - 'PROJECT_ID=$PROJECT_ID'\n",
    "    - 'SHORT_SHA=$SHORT_SHA'\n",
    "    - 'REPO_NAME=$REPO_NAME'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-rhythm",
   "metadata": {},
   "source": [
    "The same cloudbuild definition can be used by all the projects following the same folder structure, independently of the number of components. Note that we kept the same images, python:3.8-slim and docker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-occasion",
   "metadata": {},
   "source": [
    "If you go through the bash scripts in deployment/gcp/,you can notice we add two tags to our Docker images before pushing them to GCR:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-theory",
   "metadata": {},
   "source": [
    "```\n",
    "image_name=\"${folder%?}\"\n",
    "docker build -t gcr.io/$PROJECT_ID/demo-mlops/$image_name:$BRANCH_NAME-$SHORT_SHA -t gcr.io/$PROJECT_ID/demo-mlops/$image_name:$BRANCH_NAME-latest .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-blind",
   "metadata": {},
   "source": [
    "This is to group the images by branch. With the previous command, we were pushing our \"component_name\" image to  **gcr.io/PROJECT_ID/demo-mlops/component_name:latest**. Every new build would automatically change the \"latest\" tag. We must have a better way to manage the tags to avoid that a pipeline in production uses the new latest image without proper testing. Adding the branch name to the tag is a way to make sure the production pipeline uses only an image that has been promoted (and tested+reviewed) to the main branch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-lodging",
   "metadata": {},
   "source": [
    "#### (Optional) One more thing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-obligation",
   "metadata": {},
   "source": [
    "We have achieved our initial goal. If we follow the folder structure that we have defined, we are able to test and build our pipeline components. There is one more thing, I would like to add for this first CI pipeline. To know the code coverage or the Docker images that have been built, we must read the Cloud Build logs to find out. Let's change that and save these reports to Google Cloud Storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-blood",
   "metadata": {},
   "source": [
    "```\n",
    "## Create your bucket.\n",
    "gsutil mb -l LOCATION gs://PROJECT_ID-mlops-deployments\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-programming",
   "metadata": {},
   "source": [
    "Let's save the artifacts in this location: cloudbuild/REPO_NAME/BRANCH_NAME/DATE/BUILD_ID/ by adding an artifact section to our cloudbuild.yaml."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-edward",
   "metadata": {},
   "source": [
    "```\n",
    "artifacts:\n",
    "    objects:\n",
    "      location: 'gs://$PROJECT_ID-mlops-deployments/cloudbuild/$REPO_NAME/$BRANCH_NAME/$(date +%Y-%m-%d-%H-%M-%S)/$BUILD_ID/'\n",
    "      paths: ['_artifacts/*']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-nomination",
   "metadata": {},
   "source": [
    "Finally, we update the bash scripts to save all the artifacts in the folder \"_artifacts/\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-score",
   "metadata": {},
   "source": [
    "Push the changes and here we are, the artifacts are being uploaded and we can easily retrieve them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-kansas",
   "metadata": {},
   "source": [
    "```\n",
    "Artifacts will be uploaded to gs://freeldom-mlops-deployments using gsutil cp\n",
    "_artifacts/*: Uploading path....\n",
    "Copying file://_artifacts/dataprep-coverage.json [Content-Type=application/json]...\n",
    "Copying file://_artifacts/images.txt [Content-Type=text/plain]...\n",
    "Copying file://_artifacts/modeltraining-coverage.json [Content-Type=application/json]...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-accordance",
   "metadata": {},
   "source": [
    "### AWS setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-humanitarian",
   "metadata": {},
   "source": [
    "For AWS, we will follow the same build logic as GCP. However, as often with AWS, it requires a bit more work on setting up the flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-contents",
   "metadata": {},
   "source": [
    "#### Cloudformation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-combine",
   "metadata": {},
   "source": [
    "We use a CloudFormation template to setup the CodeBuild job triggered on Push event. The template is in **deployment/aws/**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-queen",
   "metadata": {},
   "source": [
    "*Assumption: You already have configured CodeBuild <> GitHub integration. If not, follow the section here: https://docs.aws.amazon.com/codebuild/latest/userguide/access-tokens.html*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-producer",
   "metadata": {},
   "source": [
    "##### CodeBuild"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-station",
   "metadata": {},
   "source": [
    "This is the CodeBuild definition. We use 3 environment variables, the account ID to build the ECR repository base url. The project name (ak repo name), and the S3 bucket where we store our artifacts. We define a trigger on PUSH event then GitHub as source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-patrick",
   "metadata": {},
   "source": [
    "```\n",
    "  DeployerCodeBuild:\n",
    "    Type: \"AWS::CodeBuild::Project\"\n",
    "    Properties:\n",
    "      Artifacts:\n",
    "        Type: NO_ARTIFACTS\n",
    "      Name: !Sub \"${RepositoryName}-build\"\n",
    "      Description: !Sub \"Build ${RepositoryName}\"\n",
    "      Cache:\n",
    "        Modes:\n",
    "        - LOCAL_DOCKER_LAYER_CACHE\n",
    "        Type: LOCAL\n",
    "      Environment:\n",
    "        ComputeType: BUILD_GENERAL1_SMALL\n",
    "        Image: \"aws/codebuild/standard:4.0\"\n",
    "        PrivilegedMode: true\n",
    "        Type: LINUX_CONTAINER\n",
    "        EnvironmentVariables:\n",
    "        - Name: ACCOUNT_ID\n",
    "          Type: PLAINTEXT\n",
    "          Value: !Sub \"${AWS::AccountId}\"\n",
    "        - Name: PROJECT_NAME\n",
    "          Type: PLAINTEXT\n",
    "          Value: !Ref RepositoryName\n",
    "        - Name: DEPLOYMENT_BUCKET\n",
    "          Type: PLAINTEXT\n",
    "          Value: !Ref DeploymentBucket\n",
    "      ServiceRole: !Ref CodebuildServiceRole\n",
    "      Triggers:\n",
    "        FilterGroups:\n",
    "        - - Pattern: PUSH\n",
    "            Type: EVENT\n",
    "        Webhook: true\n",
    "      Source:\n",
    "        Auth:\n",
    "          Type: OAUTH\n",
    "        Type: GITHUB\n",
    "        GitCloneDepth: 1\n",
    "        Location: !Ref RepoCloneUrl\n",
    "        BuildSpec: |\n",
    "          version: 0.2\n",
    "          phases:\n",
    "            install:\n",
    "              runtime-versions:\n",
    "                python: 3.8\n",
    "              commands:\n",
    "                - pip3 install pipenv\n",
    "                - . ./deployment/aws/export-branch.sh\n",
    "            build:\n",
    "              commands:\n",
    "                - mkdir _artifacts && mkdir _reports\n",
    "                - aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\n",
    "                - /bin/bash deployment/aws/run-tests.sh\n",
    "                - /bin/bash deployment/aws/build-dockers.sh\n",
    "                - aws s3 cp _artifacts/ s3://$DEPLOYMENT_BUCKET/codebuild/$PROJECT_NAME/$BRANCH_NAME/$(date +%Y-%m-%d_%H:%M:%S)/$CODEBUILD_BUILD_ID/ --recursive\n",
    "          reports:\n",
    "            pytest-reports:\n",
    "              files:\n",
    "                - \"_reports/*.xml\"\n",
    "              file-format: JUNITXML\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-palestine",
   "metadata": {},
   "source": [
    "**Note the difference with GCP?** We have made the choice to add an inline build flow. This is better because we don't have to duplicate the same buidspec across all our ML repositories. Another advantage is that if the DevOps team wants to update the flow, it can do it without modifying the source repository. Let's also upload the bash scripts to S3 and download them when the job is triggered. Again, this gives the flexibility to the team to modify the CI pipeline and apply the changes to all the repositories at once (ultimately, these scripts should be version controlled as well!). It also enable the team to have different scripts for different branches (dev, stage, prod, etc.)\n",
    "\n",
    "**GCP also supports inline cloud build, so feel free to follow the same approach!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-sympathy",
   "metadata": {},
   "source": [
    "##### IAM permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-publication",
   "metadata": {},
   "source": [
    "As always with IAM, let's try to restrict the permissions to only what we need:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-browse",
   "metadata": {},
   "source": [
    "```\n",
    "CodebuildServiceRole:\n",
    "    Type: AWS::IAM::Role\n",
    "    Properties:\n",
    "      AssumeRolePolicyDocument:\n",
    "        Version: '2012-10-17'\n",
    "        Statement:\n",
    "          - Effect: Allow\n",
    "            Principal:\n",
    "              Service:\n",
    "                - codebuild.amazonaws.com\n",
    "            Action:\n",
    "              - sts:AssumeRole\n",
    "      Path: '/'\n",
    "      RoleName: !Sub \"${RepositoryName}-build-service-role\"\n",
    "      Policies:\n",
    "        - PolicyName: cfn-deployer-codebuild-service-policy\n",
    "          PolicyDocument:\n",
    "            Version: '2012-10-17'\n",
    "            Statement:\n",
    "              - Effect: Allow\n",
    "                Action:\n",
    "                  - logs:CreateLogGroup\n",
    "                  - logs:CreateLogStream\n",
    "                  - logs:PutLogEvents\n",
    "                Resource:\n",
    "                  - !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/codebuild/${RepositoryName}-build:*'\n",
    "              - Effect: Allow\n",
    "                Action:\n",
    "                  - codebuild:CreateReportGroup\n",
    "                  - codebuild:CreateReport\n",
    "                  - codebuild:BatchPutTestCases\n",
    "                  - codebuild:UpdateReport\n",
    "                Resource:\n",
    "                  - !Sub 'arn:aws:codebuild:${AWS::Region}:${AWS::AccountId}:report-group/${RepositoryName}-build-pytest-reports'\n",
    "              - Effect: Allow\n",
    "                Action:\n",
    "                  - 's3:List*'\n",
    "                Resource:\n",
    "                  - !Sub 'arn:aws:s3:::${DeploymentBucket}'\n",
    "              - Effect: Allow\n",
    "                Action:\n",
    "                  - s3:GetObject\n",
    "                Resource:\n",
    "                  - !Sub 'arn:aws:s3:::${DeploymentBucket}/*'\n",
    "              - Effect: Allow\n",
    "                Action:\n",
    "                  - s3:PutObject\n",
    "                Resource:\n",
    "                  - !Sub 'arn:aws:s3:::${DeploymentBucket}/codebuild/${RepositoryName}/*'\n",
    "              - Effect: Allow\n",
    "                Action:\n",
    "                  - ecr:GetAuthorizationToken\n",
    "                Resource:\n",
    "                  - '*'\n",
    "              - Effect: Allow\n",
    "                Action:\n",
    "                  - ecr:BatchCheckLayerAvailability\n",
    "                  - ecr:PutImage\n",
    "                  - ecr:BatchGetImage\n",
    "                  - ecr:ListImages\n",
    "                  - ecr:DescribeImages\n",
    "                  - ecr:DescribeRepositories\n",
    "                  - ecr:CreateRepository\n",
    "                  - ecr:InitiateLayerUpload\n",
    "                  - ecr:UploadLayerPart\n",
    "                  - ecr:CompleteLayerUpload\n",
    "                Resource:\n",
    "                  - !Sub 'arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/${RepositoryName}/*'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-coordinate",
   "metadata": {},
   "source": [
    "#### Bash scripts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-civilian",
   "metadata": {},
   "source": [
    "The bash scripts for AWS follow the same logic as the ones for GCP, and there are only minor changes, especially to push the images to AWS ECR. Upload them to S3 as shown on the BuildSpec definition. I remind you that ultimately, these scripts should also be pushed to a Git repository before being deployed on S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-amazon",
   "metadata": {},
   "source": [
    "Deploy the CloudFormation template. The parameters should be:\n",
    "1. RepositoryName: Name of your GitHub repository\n",
    "2. RepoCloneUrl: HTTPS Clone URL of your repository\n",
    "3. DeploymentBucket: Bucket used to store the deployment scripts. If you don't have any, deploy the CloudFormation template **bucket.yaml** first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-complement",
   "metadata": {},
   "source": [
    "Once deployed, you should see a new CodeBuild job has been created. Try to push a change to your repository and you should see a new build being triggered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-angel",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-relation",
   "metadata": {},
   "source": [
    "We have seen how to create a simple Continuous Integration for your ML projects. We have used GCP Cloud Build and AWS CodeBuild, but there are other tools and products that you can explore. This demo repository is a just an example to get you started and you can enhance it in many ways, depending on your own requirements. Keep in mind this is only for CI, and in a future article, we will see how to include CD.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
